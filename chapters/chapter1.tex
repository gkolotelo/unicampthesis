%!TEX root = ../main.tex
% This is an example first chapter.  You should put chapters/appendices that you
% write into separate files, and add a \include{yourfilename} to
% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.

\chapter{Introduction}

% Epigraph example:
\epi{Learn from yesterday, live for today, hope for tomorrow. The important thing is not to stop questioning.}{Albert Einstein}

Micro-optimization is a technique to reduce the overall operation count of
floating point operations.  In a standard floating point unit, floating
point operations are fairly high level, such as ``multiply'' and ``add'';
in a micro floating point unit ($\mu$FPU), these have been broken down into
their constituent low-level floating point operations on the mantissas and
exponents of the floating point numbers.

Chapter two describes the architecture of the $\mu$FPU unit, and the
motivations for the design decisions made.

Chapter three describes the design of the compiler, as well as how the
optimizations discussed in section were implemented.

Chapter four describes the purpose of test code that was compiled, and which
statistics were gathered by running it through the simulator.  The purpose
is to measure what effect the micro-optimizations had, compared to
unoptimized code.  Possible future expansions to the project are also
discussed.

\section{Motivations for micro-optimization}

The idea of micro-optimization is motivated by the recent trends in computer
architecture towards low-level parallelism and small, pipelineable
instruction sets \cite{misc-full,inbook-full}.  By getting rid of more
complex instructions and concentrating on optimizing frequently used
instructions, substantial increases in performance were realized.

Another important motivation was the trend towards placing more of the
burden of performance on the compiler.  Many of the new architectures depend
on an intelligent, optimizing compiler in order to realize anywhere near
their peak performance
\cite{article-full,mastersthesis-full,unpublished-full}.  In these cases, the
compiler not only is responsible for faithfully generating native code to
match the source language, but also must be aware of instruction latencies,
delayed branches, pipeline stages, and a multitude of other factors in order
to generate fast code \cite{article-full}.

\begin{figure}[!ht]
	\centering
	\missingfigure{todonotes package}
	\caption{I'm a missing figure.}
	\label{ch1:figure1}
\end{figure}

Taking these ideas one step further, it seems that the floating point
operations that are normally single, large instructions can be further broken
down into smaller, simpler, faster instructions, with more control in the
compiler and less in the hardware, see Figure \ref{ch1:figure1}.  This is the idea behind a
micro-optimizing FPU; break the floating point instructions down into their
basic components and use a small, fast implementation, with a large part of
the burden of hardware allocation and optimization shifted towards
compile-time.

% This is an example of the usage of the package minted for code snippets
% \begin{listing}[!ht]
%   \begin{minted}[linenos, autogobble, bgcolor=bg, firstnumber=5, frame=lines]{matlab}
%       wp=0.2*pi/pi;
%       ws=0.3*pi/pi;
%       Rp=-20*log10(0.9);  % Ripple
%       Rs=20*log10(0.001); % Atenuação mínima
% 
%       [n_butter,wn_butter] = buttord(wp,ws,Rp,Rs)* buttord(wp,ws,Rp,Rs);
% 
%       [n_cheby,wn_cheby] = cheb1ord(wp,ws,Rp,Rs);
%   \end{minted}
%   \caption{Code Snippet.}
% \end{listing}

Along with the hardware speedups possible by using a $\mu$FPU, there are
also optimizations that the compiler can perform on the code that is
generated.  In a normal sequence of floating point operations, there are
many hidden redundancies that can be eliminated by allowing the compiler to
control the floating point operations down to their lowest level.

These optimizations are described in detail in section \ref{ch1:opts}.

\section{Description of micro-optimization}\label{ch1:opts}

In order to perform a sequence of floating point operations, a normal FPU
performs many redundant internal shifts and normalizations in the process of
performing a sequence of operations.  However, if a compiler can
decompose the floating point operations it needs down to the lowest level,
it then can optimize away many of these redundant operations \cite{inproceedings-full}.  

If there is some additional hardware support specifically for
micro-optimization, there are additional optimizations that can be
performed.  This hardware support entails extra ``guard bits'' on the
standard floating point formats, to allow several unnormalized operations to
be performed in a row without the loss information\footnote{A description of the floating point format used is shown in figures and.}.  A discussion of the mathematics behind
unnormalized arithmetic is in appendix.

The optimizations \todo{This is a to-do list\ldots} that the compiler can perform fall into several categories:

\subsection{Post Multiply Normalization}

When more than two multiplications are performed in a row, the intermediate
normalization of the results between multiplications can be eliminated.
This is because with each multiplication, the mantissa can become
denormalized by at most one bit.  If there are guard bits on the mantissas
to prevent bits from ``falling off'' the end during multiplications, the
normalization can be postponed until after a sequence of several
multiplies. Using unnormalized numbers for math is not a new idea; a
good example of it is the Control Data CDC 6600, designed by Seymour Cray.
\cite{techreport-full,misc-full} The CDC 6600 had all of its instructions performing
unnormalized arithmetic, with a separate {\tt NORMALIZE} instruction.

\begin{table}[!ht]\label{table1}
	\centering
	\begin{tabular}[b]{|c|c|}\hline
		Table head  & Table head  \\ \hline
		Some values & Some values \\
		Some values & Some values \\
		Some values & Some values \\
		Some values & Some values \\
		Some values & Some values \\
		Some values & Some values \\ \hline
	\end{tabular}
	\caption{I'm a table.}
\end{table}

As you can see, the intermediate results can be multiplied together, with no
need for intermediate normalizations due to the guard bit.  It is only at
the end of the operation that the normalization must be performed, in order
to get it into a format suitable for storing in memory \cite{proceedings-full}.

\subsubsection{Block Exponent}

In a unoptimized sequence of additions, the sequence of operations is as
follows for each pair of numbers ($m_1$,$e_1$) and ($m_2$,$e_2$).
\begin{enumerate}
  \item Compare $e_1$ and $e_2$.
  \item Shift the mantissa associated with the smaller exponent $|e_1-e_2|$
        places to the right.
  \item Add $m_1$ and $m_2$.
  \item Find the first one in the resulting mantissa.
  \item Shift the resulting mantissa so that normalized
  \item Adjust the exponent accordingly.
\end{enumerate}

Out of 6 steps, only one is the actual addition, and the rest are involved
in aligning the mantissas prior to the add, and then normalizing the result
afterward.  In the block exponent optimization, the largest mantissa is
found to start with, and all the mantissa's shifted before any additions
take place.  Once the mantissas have been shifted, the additions can take
place one after another. This requires that for n consecutive
additions, there are $\log_{2}n$ high guard bits to prevent overflow.  In
the $\mu$FPU, there are 3 guard bits, making up to 8 consecutive additions
possible. An example of the Block Exponent optimization on the expression
X = A + B + C is given in figure.






